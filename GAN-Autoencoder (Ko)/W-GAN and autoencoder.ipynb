{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "import time\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn.datasets\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.utils import make_grid\n",
    "import torchvision\n",
    "\n",
    "import scipy.misc\n",
    "from scipy.misc import imsave\n",
    "from IPython.display import display, clear_output\n",
    "from pyro.distributions.relaxed_straight_through import RelaxedBernoulliStraightThrough\n",
    "from torch.distributions.bernoulli import Bernoulli\n",
    "from torch.utils.data import RandomSampler, BatchSampler\n",
    "from skimage.measure import compare_ssim as ssim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    gpu = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM = 64 # Model dimensionality\n",
    "BATCH_SIZE = 50 # Batch size\n",
    "LAMBDA = 10 # Gradient penalty lambda hyperparameter\n",
    "OUTPUT_DIM = 784 # Number of pixels in MNIST (28*28)\n",
    "DOWNLOAD_MNIST = False\n",
    "\n",
    "FEATURE_LENGTH = 256 # How many binary values the encoded stage has\n",
    "LOAD_MODEL = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = './datasets/mnist/'\n",
    "\n",
    "train_data = datasets.MNIST(\n",
    "    root=root,\n",
    "    transform=torchvision.transforms.ToTensor(),\n",
    "    download=DOWNLOAD_MNIST,\n",
    "    train=True,\n",
    ")\n",
    "test_data = datasets.MNIST(\n",
    "    root=root,\n",
    "    transform=torchvision.transforms.ToTensor(),\n",
    "    download=DOWNLOAD_MNIST,\n",
    "    train=False,\n",
    ")\n",
    "\n",
    "# Data Loader for easy mini-batch return in training, the image batch shape will be (50, 1, 28, 28)\n",
    "trainloader = torch.utils.data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(dataset=test_data, batch_size=100)\n",
    "\n",
    "\n",
    "for display_data,y in testloader:\n",
    "    break\n",
    "save_image(display_data, \"images/_original.png\", nrow=10, normalize=False)\n",
    "\n",
    "display_data = display_data.view(-1, 1, 28*28)\n",
    "if use_cuda:\n",
    "    display_data = display_data.cuda(gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(28*28, 256),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(256, 256),\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(256, 256),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(256, 28*28),\n",
    "            nn.Sigmoid(),       # compress to a range (0, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, image_batch):\n",
    "        \n",
    "        encoded = self.encoder(image_batch)\n",
    "        bernoulli_encoded = RelaxedBernoulliStraightThrough(0.7, logits=encoded).rsample()\n",
    "        decoded = self.decoder(bernoulli_encoded)\n",
    "        return decoded\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        main = nn.Sequential(\n",
    "            nn.Conv2d(1, DIM, 5, stride=2, padding=2),\n",
    "            # nn.Linear(OUTPUT_DIM, 4*4*4*DIM),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(DIM, 2*DIM, 5, stride=2, padding=2),\n",
    "            # nn.Linear(4*4*4*DIM, 4*4*4*DIM),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(2*DIM, 4*DIM, 5, stride=2, padding=2),\n",
    "            # nn.Linear(4*4*4*DIM, 4*4*4*DIM),\n",
    "            nn.ReLU(True),\n",
    "            # nn.Linear(4*4*4*DIM, 4*4*4*DIM),\n",
    "            # nn.LeakyReLU(True),\n",
    "            # nn.Linear(4*4*4*DIM, 4*4*4*DIM),\n",
    "            # nn.LeakyReLU(True),\n",
    "        )\n",
    "        self.main = main\n",
    "        self.output = nn.Linear(4*4*4*DIM, 1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = input.view(-1, 1, 28, 28)\n",
    "        out = self.main(input)\n",
    "        out = out.view(-1, 4*4*4*DIM)\n",
    "        out = self.output(out)\n",
    "        return out.view(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_gradient_penalty(netD, real_data, fake_data):\n",
    "    #print real_data.size()\n",
    "    alpha = torch.rand(BATCH_SIZE, 1)\n",
    "    alpha = alpha.expand(real_data.size())\n",
    "    alpha = alpha.cuda(gpu) if use_cuda else alpha\n",
    "\n",
    "    interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n",
    "\n",
    "    if use_cuda:\n",
    "        interpolates = interpolates.cuda(gpu)\n",
    "    interpolates = autograd.Variable(interpolates, requires_grad=True)\n",
    "\n",
    "    disc_interpolates = netD(interpolates)\n",
    "\n",
    "    gradients = autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n",
    "                              grad_outputs=torch.ones(disc_interpolates.size()).cuda(gpu) if use_cuda else torch.ones(\n",
    "                                  disc_interpolates.size()),\n",
    "                              create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * LAMBDA\n",
    "    return gradient_penalty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myplot(x,y, name):\n",
    "    plt.clf()\n",
    "    plt.plot(x, y, 'ro')\n",
    "    plt.xlabel('iteration')\n",
    "    plt.ylabel(name)\n",
    "    plt.savefig('images/'+name+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Iteration 59999 , epoch 49 ,step 6/7 ,increase 0.015625'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# will generate pics for lambda 0.5 to 2**-6\n",
    "for increase in range(0, 7): \n",
    "    LOAD_MODEL = False\n",
    "    #-----------------------------------------------------------\n",
    "    netG = Generator()\n",
    "    netD = Discriminator()\n",
    "\n",
    "    if use_cuda:\n",
    "        netD = netD.cuda(gpu)\n",
    "        netG = netG.cuda(gpu)\n",
    "\n",
    "    optimizerD = optim.Adam(netD.parameters(), lr=1e-4, betas=(0.5, 0.9))\n",
    "    optimizerG = optim.Adam(netG.parameters(), lr=1e-4, betas=(0.5, 0.9))\n",
    "    \n",
    "    #-----------------------------------------------------------\n",
    "    if LOAD_MODEL:\n",
    "        checkpoint = torch.load('./models/lamb0epoch20.pt')\n",
    "        netG.load_state_dict(checkpoint['netG_state_dict'])\n",
    "        netD.load_state_dict(checkpoint['netD_state_dict'])\n",
    "        optimizerG.load_state_dict(checkpoint['optimizerG_state_dict'])\n",
    "        optimizerD.load_state_dict(checkpoint['optimizerD_state_dict'])\n",
    "        loaded_epoch = checkpoint['epoch']+2\n",
    "        iteration = checkpoint['iteration']\n",
    "\n",
    "        netG.eval()\n",
    "        netD.eval()\n",
    "    #-----------------------------------------------------------\n",
    "    \n",
    "    diterats = []\n",
    "    giterats = []\n",
    "    wdistarr = []\n",
    "    dcostarr = []\n",
    "    gcostarr = []\n",
    "    trainloader_length = int(len(trainloader))\n",
    "    if increase == 0:\n",
    "        DISC_LAMBDA = 0\n",
    "    else:\n",
    "        DISC_LAMBDA = 2**(-increase)# Higher -> more discriminator\n",
    "        \n",
    "    DISC_GEN_TRAIN_RATIO = 5 # How many times the discriminator should be trained for one generator train\n",
    "    EPOCHS = 50 # How many times the whole trainingset should be iterated over\n",
    "\n",
    "    if not LOAD_MODEL:\n",
    "        loaded_epoch = 0\n",
    "        iteration = 0\n",
    "\n",
    "    #-----------------------------------------------------------\n",
    "    for epoch in range(loaded_epoch, EPOCHS):\n",
    "        for x,y in trainloader:\n",
    "\n",
    "            x = x.view(-1, 28*28)\n",
    "\n",
    "            if use_cuda:\n",
    "                x = x.cuda(gpu)\n",
    "\n",
    "\n",
    "            if iteration %(DISC_GEN_TRAIN_RATIO+1) != DISC_GEN_TRAIN_RATIO:\n",
    "\n",
    "            ############################\n",
    "            # (1) Update D network\n",
    "            ###########################\n",
    "\n",
    "                # train with real\n",
    "                D_real = netD(x).mean()\n",
    "\n",
    "                # train with fake\n",
    "                fake = netG(x).detach()\n",
    "                D_fake = netD(fake).mean()\n",
    "\n",
    "                # train with gradient penalty\n",
    "                gradient_penalty = calc_gradient_penalty(netD, x, fake)\n",
    "\n",
    "                D_cost = D_fake - D_real + gradient_penalty\n",
    "\n",
    "                D_cost.backward()\n",
    "\n",
    "                Wasserstein_D = D_real - D_fake\n",
    "                optimizerD.step()\n",
    "                optimizerD.zero_grad()\n",
    "\n",
    "                diterats += [iteration]\n",
    "                dcostarr += [D_cost.item()]\n",
    "                wdistarr += [Wasserstein_D.item()]\n",
    "\n",
    "\n",
    "            if iteration %(DISC_GEN_TRAIN_RATIO+1) == DISC_GEN_TRAIN_RATIO:\n",
    "\n",
    "                ############################\n",
    "                # (2) Update G network\n",
    "                ###########################\n",
    "\n",
    "                fake = netG(x)\n",
    "                G = netD(fake).mean()\n",
    "\n",
    "                rec_x = ((fake-x)**2).sum(1).mean(0)\n",
    "    #             G_cost = rec_x - DISC_LAMBDA * G\n",
    "                G_cost = (1 - DISC_LAMBDA) * rec_x - DISC_LAMBDA * G\n",
    "\n",
    "                G_cost.backward()\n",
    "\n",
    "                optimizerG.step()\n",
    "                optimizerG.zero_grad()\n",
    "\n",
    "                # Write logs and save samples\n",
    "                giterats += [iteration]\n",
    "                gcostarr += [G_cost.item()]\n",
    "\n",
    "\n",
    "            if iteration % 100 == 99:\n",
    "                fake = netG(display_data).view(-1,1,28,28)\n",
    "                if use_cuda:\n",
    "                    fake = fake.cpu()\n",
    "                save_image(fake, \"images/%d%d.png\" % (iteration, increase), nrow=10, normalize=False)\n",
    "\n",
    "            clear_output(wait=True)\n",
    "            display('Iteration '+str(iteration)+' , epoch '+str(epoch)+' ,step '+str(increase)+'/7 ,increase '+str(DISC_LAMBDA))\n",
    "            iteration += 1\n",
    "\n",
    "        myplot(diterats, dcostarr, \"dcost%d\" % (increase))\n",
    "        myplot(diterats, wdistarr, \"wdist%d\" % (increase))\n",
    "        myplot(giterats, gcostarr, \"gcost%d\" % (increase))\n",
    "        torch.save({\n",
    "            'netG_state_dict': netG.state_dict(),\n",
    "            'netD_state_dict': netD.state_dict(),\n",
    "            'optimizerG_state_dict': optimizerG.state_dict(),\n",
    "            'optimizerD_state_dict': optimizerD.state_dict(),\n",
    "            'epoch': epoch,\n",
    "            'iteration': iteration,\n",
    "            }, './models/lamb%depoch50mnist.pt' % (increase))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
